"""
Generate an enhanced human-readable summary with document statistics
"""
import sys
import re
from pathlib import Path
from gensim.models import LdaModel
from datetime import datetime
from bs4 import BeautifulSoup

# Load the final model
CHECKPOINT_DIR = "model_checkpoints"
checkpoint_files = list(Path(CHECKPOINT_DIR).glob("final_best_model_*.pkl"))

if not checkpoint_files:
    print("ERROR: No final model found. Please run the main script first.")
    sys.exit(1)

model_file = checkpoint_files[0]
print(f"Loading model from: {model_file}")
final_model = LdaModel.load(str(model_file))

# Extract k from filename
optimal_k = int(model_file.stem.split('_k')[1])
print(f"Model has {optimal_k} topics")

# Try to extract document counts from HTML
doc_counts = {}
try:
    with open('refined_topics_summary.html', 'r', encoding='utf-8') as f:
        html_content = f.read()
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # Find all rows in the table
        rows = soup.find_all('tr')
        for row in rows:
            cells = row.find_all('td')
            if len(cells) >= 3:
                # First cell is topic ID, third cell is document count
                topic_text = cells[0].get_text()
                if 'Topic' in topic_text:
                    topic_num = int(topic_text.replace('Topic', '').strip())
                    doc_count = cells[2].get_text().strip()
                    doc_counts[topic_num] = int(doc_count)
    print(f"‚úì Loaded document counts for {len(doc_counts)} topics")
except Exception as e:
    print(f"‚ö† Could not load document counts: {e}")

# Get vocabulary size
vocab_size = len(final_model.id2word)

# Create the enhanced document
output_file = f"topics_manual_review_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"

with open(output_file, 'w', encoding='utf-8') as f:
    # Header
    f.write("‚ïî" + "‚ïê" * 78 + "‚ïó\n")
    f.write("‚ïë" + " " * 15 + "TOPIC MODEL - MANUAL CATEGORIZATION WORKSHEET" + " " * 17 + "‚ïë\n")
    f.write("‚ïö" + "‚ïê" * 78 + "‚ïù\n\n")
    
    f.write(f"Generated:        {datetime.now().strftime('%Y-%m-%d at %H:%M')}\n")
    f.write(f"Number of Topics: {optimal_k}\n")
    f.write(f"Vocabulary Size:  {vocab_size:,} terms\n")
    f.write(f"Model File:       {model_file.name}\n\n")
    
    # Quick reference
    f.write("‚îå" + "‚îÄ" * 78 + "‚îê\n")
    f.write("‚îÇ QUICK TOPIC REFERENCE                                                       ‚îÇ\n")
    f.write("‚îú" + "‚îÄ" * 78 + "‚î§\n")
    
    for topic_id in range(optimal_k):
        top_3_words = [word for word, _ in final_model.show_topic(topic_id, topn=3)]
        top_words_str = ", ".join(top_3_words)[:50]
        doc_count_str = f"({doc_counts.get(topic_id, '?')} docs)" if doc_counts else ""
        line = f"‚îÇ Topic {topic_id:2d}: {top_words_str:50s} {doc_count_str:14s} ‚îÇ\n"
        f.write(line)
    
    f.write("‚îî" + "‚îÄ" * 78 + "‚îò\n\n")
    
    # Detailed topics
    f.write("\n" + "‚ïê" * 80 + "\n")
    f.write("DETAILED TOPIC ANALYSIS\n")
    f.write("‚ïê" * 80 + "\n\n")
    
    for topic_id in range(optimal_k):
        # Get top 15 keywords for review
        top_words = final_model.show_topic(topic_id, topn=15)
        
        f.write("\n‚îå" + "‚îÄ" * 78 + "‚îê\n")
        f.write(f"‚îÇ TOPIC {topic_id:2d}" + " " * 71 + "‚îÇ\n")
        f.write("‚îú" + "‚îÄ" * 78 + "‚î§\n")
        
        if doc_counts and topic_id in doc_counts:
            f.write(f"‚îÇ Documents: {doc_counts[topic_id]:3d}" + " " * 66 + "‚îÇ\n")
            f.write("‚îú" + "‚îÄ" * 78 + "‚î§\n")
        
        # Top 5 keywords prominently
        f.write("‚îÇ TOP KEYWORDS:                                                               ‚îÇ\n")
        for i, (word, weight) in enumerate(top_words[:5], 1):
            word_display = word.replace('_', ' ')
            line = f"‚îÇ   {i}. {word_display:40s} [{weight:.3f}]" + " " * (35 - len(word_display) - len(f"{weight:.3f}")) + "‚îÇ\n"
            f.write(line)
        
        # Additional keywords (6-15)
        f.write("‚îÇ                                                                              ‚îÇ\n")
        f.write("‚îÇ Additional keywords:                                                         ‚îÇ\n")
        additional = ", ".join([word.replace('_', ' ') for word, _ in top_words[5:15]])
        # Wrap text to fit
        while additional:
            if len(additional) <= 74:
                f.write(f"‚îÇ   {additional:74s}‚îÇ\n")
                break
            else:
                # Find last comma before 74 chars
                cutoff = additional[:74].rfind(',')
                if cutoff == -1:
                    cutoff = 74
                f.write(f"‚îÇ   {additional[:cutoff+1]:74s}‚îÇ\n")
                additional = additional[cutoff+1:].strip()
        
        f.write("‚îú" + "‚îÄ" * 78 + "‚î§\n")
        f.write("‚îÇ ASSIGNED CATEGORY:                                                          ‚îÇ\n")
        f.write("‚îÇ                                                                              ‚îÇ\n")
        f.write("‚îÇ [ ] ____________________________________________                             ‚îÇ\n")
        f.write("‚îÇ                                                                              ‚îÇ\n")
        f.write("‚îú" + "‚îÄ" * 78 + "‚î§\n")
        f.write("‚îÇ QUALITY RATING:  [ ] Excellent  [ ] Good  [ ] Fair  [ ] Poor                ‚îÇ\n")
        f.write("‚îÇ                                                                              ‚îÇ\n")
        f.write("‚îÇ NOTES:                                                                       ‚îÇ\n")
        f.write("‚îÇ                                                                              ‚îÇ\n")
        f.write("‚îÇ                                                                              ‚îÇ\n")
        f.write("‚îÇ                                                                              ‚îÇ\n")
        f.write("‚îî" + "‚îÄ" * 78 + "‚îò\n")
    
    # Summary section
    f.write("\n\n" + "‚ïê" * 80 + "\n")
    f.write("REFINEMENT RECOMMENDATIONS\n")
    f.write("‚ïê" * 80 + "\n\n")
    
    f.write("1. TOPICS TO MERGE (too similar):\n")
    f.write("   Topic ___ + Topic ___ ‚Üí New category: ________________\n")
    f.write("   Topic ___ + Topic ___ ‚Üí New category: ________________\n\n")
    
    f.write("2. TOPICS TO SPLIT (too broad):\n")
    f.write("   Topic ___ ‚Üí Split into: ________________ and ________________\n")
    f.write("   Topic ___ ‚Üí Split into: ________________ and ________________\n\n")
    
    f.write("3. STOPWORDS TO ADD (appeared in multiple topics):\n")
    f.write("   - ________________\n")
    f.write("   - ________________\n")
    f.write("   - ________________\n\n")
    
    f.write("4. PARAMETER ADJUSTMENTS FOR NEXT RUN:\n")
    f.write("   Number of topics (current: {}):\n".format(optimal_k))
    f.write("   [ ] Increase to: ___\n")
    f.write("   [ ] Decrease to: ___\n")
    f.write("   [ ] Keep as is\n\n")
    
    f.write("   Other parameters to adjust:\n")
    f.write("   - MIN_DOC_COUNT (current: 5): ___\n")
    f.write("   - MAX_DOC_FRACTION (current: 0.85): ___\n\n")
    
    f.write("5. OVERALL ASSESSMENT:\n")
    f.write("   Quality of topic separation:  [ ] Excellent [ ] Good [ ] Fair [ ] Poor\n")
    f.write("   Coverage of domain:           [ ] Complete  [ ] Good [ ] Fair [ ] Poor\n")
    f.write("   Actionability for analysis:   [ ] High      [ ] Medium [ ] Low\n\n")

print(f"\n‚úÖ Enhanced review document created: {output_file}")
print(f"   Location: {Path(output_file).resolve()}")
print(f"\nüìã This document includes:")
print("   ‚úì Quick reference of all topics")
print("   ‚úì Detailed keywords for each topic")
print("   ‚úì Document counts per topic")
print("   ‚úì Checkboxes for categorization")
print("   ‚úì Quality rating system")
print("   ‚úì Refinement recommendations section")
print(f"\nüí° Ready to print and annotate!")

