# BERTopic Analysis Outputs

This directory contains all outputs from BERTopic topic modeling analyses on the microalgae biofuel literature corpus.

## Overview

BERTopic analysis was run on 223 scientific papers to discover semantic topics and thematic structure. All results, models, and visualizations are stored here with timestamps to track different analysis runs.

## Directory Contents

### Timestamped Files

All files include timestamps in format `YYYYMMDD_HHMMSS` to track when analysis was run.

#### Core Output Files

**`topic_info_YYYYMMDD_HHMMSS.csv`**
- Complete topic information
- Columns: Topic ID, Count, Name, Representation, Keywords
- Top keywords ranked by c-TF-IDF scores
- One row per discovered topic

**`document_topics_YYYYMMDD_HHMMSS.csv`**
- Document-level topic assignments
- Columns: filename, text, topic, topic_probability, topic_name
- One row per analyzed document
- Use for filtering papers by topic

**`raw_extractions_YYYYMMDD_HHMMSS.csv`**
- Raw text extracted from PDFs
- Full document text before preprocessing
- Useful for validation and debugging

**`BERTOPIC_SUMMARY_YYYYMMDD_HHMMSS.txt`**
- Human-readable summary report
- Topic descriptions and statistics
- Representative papers per topic
- Analysis metadata and parameters

#### Interactive Visualizations (HTML)

**`intertopic_distance_YYYYMMDD_HHMMSS.html`**
- Interactive 2D topic map
- Circle size = topic prevalence
- Distance between topics = semantic similarity
- Hover to see keywords and details
- **Open in web browser to explore**

**`hierarchy_YYYYMMDD_HHMMSS.html`**
- Hierarchical clustering dendrogram
- Shows topic relationships at multiple levels
- Identify higher-order thematic categories
- Interactive - click to expand/collapse

**`barchart_YYYYMMDD_HHMMSS.html`**
- Topic keyword bar charts
- Top keywords per topic with importance scores
- C-TF-IDF values for each term
- Compare keyword relevance across topics

#### Saved Model

**`bertopic_model_YYYYMMDD_HHMMSS/`** (directory)
- Complete trained BERTopic model
- Can be reloaded for further analysis
- Contains:
  - `config.json` - Model configuration
  - `topics.json` - Topic definitions
  - `topic_embeddings.bin` - Topic vector representations
  - `ctfidf.bin` - c-TF-IDF model
  - `ctfidf_config.json` - Vectorizer configuration

### Subdirectories

**`visualizations/`**
- Static PNG visualizations
- Publication-quality plots
- Generated by supplementary visualization scripts

**`bertopic_model_YYYYMMDD_HHMMSS/`**
- Multiple saved models from different runs
- Each contains complete model state
- Enables reproducibility and comparison

## Latest Analysis

**Date**: November 19, 2025 13:02:32  
**Files**: All files timestamped `20251119_130232`

### Key Results (Latest Run)

**Documents Processed**: 223  
**Topics Discovered**: 6  
**Average Confidence**: 60.6%  
**Outliers**: 0%  

**Topics Identified**:
1. **Topic 0** - General Microalgae Biofuel Production & Challenges (67 docs, 30.0%)
2. **Topic 1** - Wastewater Treatment & Circular Bioeconomy (43 docs, 19.3%)
3. **Topic 2** - Integrated Biorefinery & Value-Added Products (34 docs, 15.2%)
4. **Topic 3** - Sustainability Assessment & Policy Frameworks (31 docs, 13.9%)
5. **Topic 4** - Biodiesel Production Technology (27 docs, 12.1%)
6. **Topic 5** - Third-Generation Biofuels & Future Innovations (21 docs, 9.4%)

## Using the Outputs

### Loading Results in Python

```python
import pandas as pd
from bertopic import BERTopic

# Load topic information
topic_info = pd.read_csv('topic_info_20251119_130232.csv')
print(topic_info.head())

# Load document assignments
doc_topics = pd.read_csv('document_topics_20251119_130232.csv')

# Filter papers by topic
topic_0_papers = doc_topics[doc_topics['topic'] == 0]
print(f"Topic 0 has {len(topic_0_papers)} papers")

# Load saved model for further analysis
model = BERTopic.load("bertopic_model_20251119_130232")

# Get keywords for specific topic
keywords = model.get_topic(0)
print("Top keywords for Topic 0:", keywords[:10])

# Predict topic for new document
new_abstract = ["Microalgae cultivation for sustainable biofuel production..."]
topics, probs = model.transform(new_abstract)
print(f"Predicted topic: {topics[0]} (confidence: {probs[0].max():.2%})")
```

### Opening Visualizations

**Windows:**
```powershell
# Open in default browser
Start-Process intertopic_distance_20251119_130232.html
Start-Process hierarchy_20251119_130232.html
Start-Process barchart_20251119_130232.html
```

**Linux/Mac:**
```bash
# Open in browser
open intertopic_distance_20251119_130232.html  # Mac
xdg-open intertopic_distance_20251119_130232.html  # Linux
```

### Analyzing Document Assignments

```python
import pandas as pd
import matplotlib.pyplot as plt

# Load document topics
df = pd.read_csv('document_topics_20251119_130232.csv')

# Topic distribution
topic_counts = df['topic'].value_counts().sort_index()
print(topic_counts)

# Confidence distribution
plt.figure(figsize=(10, 6))
df['topic_probability'].hist(bins=30)
plt.xlabel('Topic Probability (Confidence)')
plt.ylabel('Number of Documents')
plt.title('Distribution of Topic Assignment Confidence')
plt.savefig('confidence_distribution.png', dpi=300)

# Average confidence per topic
avg_conf = df.groupby('topic')['topic_probability'].mean()
print("Average confidence per topic:")
print(avg_conf)
```

### Extracting Papers by Topic

```python
import pandas as pd

df = pd.read_csv('document_topics_20251119_130232.csv')

# Get all papers in Topic 3 (Sustainability & Policy)
sustainability_papers = df[df['topic'] == 3].copy()

# Sort by confidence
sustainability_papers = sustainability_papers.sort_values(
    'topic_probability', 
    ascending=False
)

# Export for literature review
sustainability_papers[['filename', 'topic_probability']].to_csv(
    'sustainability_policy_papers.csv',
    index=False
)

print(f"Found {len(sustainability_papers)} papers on sustainability & policy")
print(f"Top 5 most confident assignments:")
print(sustainability_papers[['filename', 'topic_probability']].head())
```

## File Size Information

Typical file sizes for 223 documents:

| File Type | Typical Size | Description |
|-----------|-------------|-------------|
| topic_info CSV | ~5-10 KB | Topic metadata |
| document_topics CSV | ~50-100 KB | Document assignments |
| raw_extractions CSV | ~5-10 MB | Full extracted text |
| summary TXT | ~10-20 KB | Human-readable summary |
| intertopic_distance HTML | ~500 KB | Interactive visualization |
| hierarchy HTML | ~300 KB | Dendrogram visualization |
| barchart HTML | ~400 KB | Keyword charts |
| bertopic_model folder | ~50-100 MB | Complete saved model |

**Total directory size**: ~60-120 MB per analysis run

## Comparing Different Runs

If you've run the analysis multiple times:

```python
import pandas as pd

# Load two different runs
run1 = pd.read_csv('topic_info_20251115_120000.csv')
run2 = pd.read_csv('topic_info_20251119_130232.csv')

# Compare number of topics
print(f"Run 1: {len(run1)} topics")
print(f"Run 2: {len(run2)} topics")

# Compare top keywords
print("Run 1 Topic 0:", run1.loc[0, 'Representation'])
print("Run 2 Topic 0:", run2.loc[0, 'Representation'])
```

## Cleaning Up Old Runs

To save space, you can delete old analysis outputs:

```powershell
# List all runs by timestamp
Get-ChildItem *_*.csv, *_*.html, *_*.txt | 
    Select-Object Name, CreationTime, Length |
    Sort-Object CreationTime -Descending

# Keep only the latest run
# Delete older files manually or with script
# BE CAREFUL - this deletes data permanently!

# Archive old runs instead
$timestamp = "20251115_120000"  # Old run to archive
Compress-Archive -Path *_$timestamp.* -DestinationPath "archived_run_$timestamp.zip"
Remove-Item *_$timestamp.*  # Only after confirming archive created
```

## Integration with Other Analyses

### Cross-referencing with Bibliometric Data

```python
import pandas as pd

# Load BERTopic results
bertopic_results = pd.read_csv('document_topics_20251119_130232.csv')

# Load bibliometric data (if filenames match)
biblio_data = pd.read_csv('../bibliometric-analysis/filtered_data.csv')

# Merge on filename (if applicable)
# This enables analysis like:
# - Which topics are most cited?
# - Which topics have highest h-index authors?
# - Geographic distribution by topic
```

### Preparing for Thesis

1. **Export topic summaries** - Use for methodology section
2. **Save visualizations** - Include in results chapter
3. **Document assignments** - Organize literature review by topic
4. **Model parameters** - Report in methods

## Summary Reports

The `BERTOPIC_SUMMARY_*.txt` files contain:
- Analysis overview and parameters
- Discovered topics with descriptions
- Representative papers per topic
- Topic distribution statistics
- Confidence metrics
- Interpretation and insights

**Recommended for:**
- Quick overview without opening multiple files
- Sharing results with supervisors
- Including in appendices
- Reference during writing

## Troubleshooting

### Can't open HTML files

**Solution:**
- Right-click → Open with → Web browser
- Or drag file into open browser window
- If still not working, check file isn't corrupted (>0 bytes)

### CSV files won't open in Excel

**Solution:**
- Files may be too large or have encoding issues
- Use Python pandas or R instead
- Or open in text editor first to check format

### Model won't load

**Error: "No such file or directory"**
```python
import os
# Check model directory exists
model_path = "bertopic_model_20251119_130232"
print(os.path.exists(model_path))
print(os.listdir(model_path))  # Should show config files
```

### Files missing after script run

**Check:**
1. Script completed successfully (check terminal output)
2. No errors during save phase
3. Correct working directory when running script
4. Sufficient disk space

## Best Practices

**Version Control:**
- Do NOT commit large output files to Git
- Add to `.gitignore`:
  ```
  bertopic_outputs/*.csv
  bertopic_outputs/*.html
  bertopic_outputs/bertopic_model_*/
  bertopic_outputs/*.txt
  ```
- Keep only summary reports or compress archives

**Backup:**
- Archive important runs before deleting
- Keep at least the latest run intact
- Document which run was used for thesis/publications

**Documentation:**
- Note analysis parameters used
- Record any custom preprocessing
- Document interpretation decisions

## References

**BERTopic Documentation:**
- https://maartengr.github.io/BERTopic/
- API Reference: https://maartengr.github.io/BERTopic/api/bertopic.html

**Understanding Outputs:**
- c-TF-IDF: Class-based TF-IDF for topic representation
- Topic probability: Confidence of document-topic assignment
- Outlier topic (-1): Documents that don't fit any cluster

---

**Last Updated**: November 22, 2025  
**Latest Analysis**: November 19, 2025 13:02:32  
**Total Outputs**: Multiple timestamped runs  
**Status**: Analysis complete, results ready for interpretation

