{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Install Required Packages\n",
        "\n",
        "Run this cell first to install all required packages. This may take a few minutes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# WARNING: Python 3.14 is NOT supported by these packages\n",
        "# You MUST use Python 3.11 or 3.12 for this to work\n",
        "# These installations will likely fail on Python 3.14\n",
        "\n",
        "import sys\n",
        "print(f\"Current Python version: {sys.version}\")\n",
        "print(\"Required: Python 3.10 - 3.13\")\n",
        "\n",
        "%pip install PyMuPDF scikit-learn networkx pyvis pandas sentence-transformers plotly\n",
        "%pip install numpy scipy tqdm\n",
        "%pip install llvmlite numba\n",
        "%pip install hdbscan umap-learn\n",
        "%pip install bertopic\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# PDF Topic Knowledge Graph using BERTopic\n",
        "\n",
        "This notebook extracts text from PDF files, trains a BERTopic model to discover topics, and creates an interactive knowledge graph visualization showing the relationships between documents, topics, and keywords.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Import Required Libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import fitz  # PyMuPDF\n",
        "import glob\n",
        "from pathlib import Path\n",
        "from bertopic import BERTopic\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import networkx as nx\n",
        "from pyvis.network import Network\n",
        "import pandas as pd\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuration\n",
        "\n",
        "Set your parameters here:\n",
        "- **PDF_FOLDER_PATH**: Path to your PDF files\n",
        "- **OUTPUT_FILENAME**: Name for the output HTML file\n",
        "- **MIN_TOPIC_SIZE**: Minimum documents per topic (lower = more topics, higher = fewer topics)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Configuration ---\n",
        "PDF_FOLDER_PATH = r\"D:\\Github\\phd\\ML\\included\" \n",
        "OUTPUT_FILENAME = \"pdf_knowledge_graph.html\"\n",
        "MIN_TOPIC_SIZE = 10  # Adjust this to get more (lower) or fewer (higher) topics\n",
        "\n",
        "print(\"--- Script Started ---\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Extract Text from PDFs\n",
        "\n",
        "Read all PDF files from the specified folder and extract their text content.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"\\n--- Step 1: Extracting Text from PDFs in '{PDF_FOLDER_PATH}' ---\")\n",
        "\n",
        "pdf_files = list(Path(PDF_FOLDER_PATH).glob(\"*.pdf\"))\n",
        "print(f\"Found {len(pdf_files)} PDF files.\")\n",
        "\n",
        "documents = []  # List to hold the text of each PDF\n",
        "doc_names = []    # List to hold the filenames\n",
        "\n",
        "for pdf_path in pdf_files:\n",
        "    try:\n",
        "        doc = fitz.open(pdf_path)\n",
        "        full_text = \"\"\n",
        "        \n",
        "        for page_num in range(doc.page_count):\n",
        "            page = doc.load_page(page_num)\n",
        "            full_text += page.get_text(\"text\")\n",
        "            \n",
        "        documents.append(full_text)\n",
        "        doc_names.append(pdf_path.name)  # Store just the filename\n",
        "        doc.close()\n",
        "    except Exception as e:\n",
        "        print(f\"  [Error] Failed to read {pdf_path}: {e}\")\n",
        "\n",
        "print(f\"Successfully extracted text from {len(documents)} documents.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Train BERTopic Model\n",
        "\n",
        "Train the BERTopic model to discover topics in the documents. This is the most time-consuming step.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if documents:\n",
        "    print(f\"\\n--- Step 2: Training BERTopic Model (min_topic_size={MIN_TOPIC_SIZE}) ---\")\n",
        "\n",
        "    # Create a vectorizer to remove stop words and find 1- and 2-word phrases\n",
        "    vectorizer_model = CountVectorizer(stop_words=\"english\", ngram_range=(1, 2))\n",
        "\n",
        "    # Initialize BERTopic\n",
        "    topic_model = BERTopic(\n",
        "        vectorizer_model=vectorizer_model,\n",
        "        embedding_model=\"sentence-transformers/all-mpnet-base-v2\",\n",
        "        min_topic_size=MIN_TOPIC_SIZE,\n",
        "        language=\"english\",\n",
        "        verbose=True \n",
        "    )\n",
        "\n",
        "    # Fit the model to the extracted text\n",
        "    # This is the most time-consuming step\n",
        "    topics, probabilities = topic_model.fit_transform(documents)\n",
        "\n",
        "    print(\"BERTopic model training complete.\")\n",
        "else:\n",
        "    print(\"\\nNo documents were successfully extracted. Exiting.\")\n",
        "    raise Exception(\"No documents to process\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Prepare Data for Knowledge Graph\n",
        "\n",
        "Extract document-to-topic mappings, topic keywords, and other metadata from the trained model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n--- Step 3: Preparing Data for Knowledge Graph ---\")\n",
        "\n",
        "# 1. Get Document-to-Topic Mappings (using our filenames)\n",
        "print(\"Getting document info...\")\n",
        "doc_info = topic_model.get_document_info(documents, doc_names=doc_names)\n",
        "\n",
        "# 2. Get Topic-to-Keyword Mappings\n",
        "print(\"Getting topic keywords...\")\n",
        "topic_keywords = topic_model.get_topics()\n",
        "\n",
        "# 3. Get General Topic Info (for names, sizes, etc.)\n",
        "topic_info = topic_model.get_topic_info()\n",
        "\n",
        "print(f\"Loaded {len(doc_info)} documents and {len(topic_info)} topics (including outliers).\")\n",
        "\n",
        "# Display topic information\n",
        "print(\"\\nTopic Overview:\")\n",
        "display(topic_info.head(10))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Build the Knowledge Graph\n",
        "\n",
        "Create a NetworkX graph with:\n",
        "- **Topic nodes** (red)\n",
        "- **Document nodes** (blue)\n",
        "- **Keyword nodes** (green)\n",
        "- **Edges** connecting documents to topics, topics to keywords, and topics to related topics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n--- Step 4: Building Knowledge Graph with NetworkX ---\")\n",
        "\n",
        "# Initialize the Graph\n",
        "G = nx.Graph()\n",
        "\n",
        "# --- 1. Add Topic Nodes ---\n",
        "print(\"  Adding Topic nodes...\")\n",
        "for index, row in topic_info.iterrows():\n",
        "    topic_id = row['Topic']\n",
        "    if topic_id == -1:  # Skip the outlier topic\n",
        "        continue\n",
        "        \n",
        "    G.add_node(\n",
        "        f\"Topic_{topic_id}\", \n",
        "        type='topic', \n",
        "        size=max(row['Count'] / 5, 10),  # Scale size, ensure min size 10\n",
        "        title=f\"Topic {topic_id}: {row['Name']}\",  # Hover-over text\n",
        "        color='#f08080'  # Light red for topics\n",
        "    )\n",
        "\n",
        "# --- 2. Add Document Nodes and Document-Topic Edges ---\n",
        "print(\"  Adding Document nodes and edges...\")\n",
        "for index, row in doc_info.iterrows():\n",
        "    topic_id = row['Topic']\n",
        "    if topic_id == -1:  # Skip outlier documents\n",
        "        continue\n",
        "    \n",
        "    doc_name = row['Name']\n",
        "    \n",
        "    # Add the document node\n",
        "    G.add_node(\n",
        "        doc_name, \n",
        "        type='document', \n",
        "        size=5, \n",
        "        title=doc_name,\n",
        "        color='#87ceeb'  # Sky blue for documents\n",
        "    )\n",
        "    \n",
        "    # Add the edge connecting the document to its topic\n",
        "    G.add_edge(doc_name, f\"Topic_{topic_id}\", type='belongs_to')\n",
        "\n",
        "# --- 3. Add Keyword Nodes and Topic-Keyword Edges ---\n",
        "print(\"  Adding Keyword nodes and edges...\")\n",
        "for topic_id, keywords in topic_keywords.items():\n",
        "    if topic_id == -1:  # Skip outlier topic\n",
        "        continue\n",
        "        \n",
        "    for keyword, score in keywords:\n",
        "        G.add_node(\n",
        "            keyword, \n",
        "            type='keyword', \n",
        "            size=max(score * 100, 5),  # Scale keyword size, ensure min size 5\n",
        "            title=f\"Keyword: {keyword}\",\n",
        "            color='#90ee90'  # Light green for keywords\n",
        "        )\n",
        "        \n",
        "        G.add_edge(\n",
        "            f\"Topic_{topic_id}\", \n",
        "            keyword, \n",
        "            type='has_keyword', \n",
        "            weight=score \n",
        "        )\n",
        "\n",
        "# --- 4. (Optional) Add Topic-to-Topic Edges ---\n",
        "print(\"  Calculating and adding topic-to-topic similarity edges...\")\n",
        "try:\n",
        "    similarity_matrix = topic_model.topic_similarity_matrix()\n",
        "    topic_ids = topic_info['Topic'].tolist()\n",
        "\n",
        "    for i in range(len(topic_ids)):\n",
        "        for j in range(i + 1, len(topic_ids)):\n",
        "            \n",
        "            topic_id_i = topic_ids[i]\n",
        "            topic_id_j = topic_ids[j]\n",
        "            \n",
        "            if topic_id_i == -1 or topic_id_j == -1:  # Skip outliers\n",
        "                continue\n",
        "                \n",
        "            similarity_score = similarity_matrix[i, j]\n",
        "            \n",
        "            # Add an edge if the similarity is above a certain threshold\n",
        "            if similarity_score > 0.1: \n",
        "                G.add_edge(\n",
        "                    f\"Topic_{topic_id_i}\", \n",
        "                    f\"Topic_{topic_id_j}\", \n",
        "                    type='related_to', \n",
        "                    weight=similarity_score,\n",
        "                    title=f\"Related (Score: {similarity_score:.2f})\",\n",
        "                    color='#cccccc'  # Gray for related topic edges\n",
        "                )\n",
        "except Exception as e:\n",
        "    print(f\"  [Warning] Could not calculate topic similarity matrix. Skipping. Error: {e}\")\n",
        "\n",
        "\n",
        "print(f\"Graph created with {G.number_of_nodes()} nodes and {G.number_of_edges()} edges.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Visualize and Save the Graph\n",
        "\n",
        "Create an interactive HTML visualization using Pyvis and save it to the output file.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"\\n--- Step 5: Visualizing Graph and Saving to '{OUTPUT_FILENAME}' ---\")\n",
        "\n",
        "# Create a pyvis network\n",
        "nt = Network(notebook=True, height='800px', width='100%', cdn_resources='in_line', heading='PDF Topic Knowledge Graph')\n",
        "\n",
        "# Load the networkx graph into pyvis\n",
        "nt.from_nx(G)\n",
        "\n",
        "# Add visualization options for better physics\n",
        "nt.set_options(\"\"\"\n",
        "var options = {\n",
        "  \"nodes\": {\n",
        "    \"font\": {\n",
        "      \"size\": 12,\n",
        "      \"face\": \"Tahoma\"\n",
        "    }\n",
        "  },\n",
        "  \"edges\": {\n",
        "    \"color\": {\n",
        "      \"inherit\": false\n",
        "    },\n",
        "    \"smooth\": false\n",
        "  },\n",
        "  \"physics\": {\n",
        "    \"barnesHut\": {\n",
        "      \"gravitationalConstant\": -40000,\n",
        "      \"centralGravity\": 0.1,\n",
        "      \"springLength\": 120,\n",
        "      \"springConstant\": 0.05\n",
        "    },\n",
        "    \"maxVelocity\": 50,\n",
        "    \"minVelocity\": 0.75,\n",
        "    \"solver\": \"barnesHut\"\n",
        "  }\n",
        "}\n",
        "\"\"\")\n",
        "\n",
        "# Save and show the interactive HTML file\n",
        "nt.show(OUTPUT_FILENAME)\n",
        "\n",
        "print(f\"\\n--- Script Finished ---\")\n",
        "print(f\"Successfully created and saved interactive knowledge graph to:\")\n",
        "print(f\"{Path(OUTPUT_FILENAME).resolve()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Optional: Explore the Results\n",
        "\n",
        "You can explore the trained model further using these commands:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# View all topics\n",
        "topic_info\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# View documents and their assigned topics\n",
        "doc_info[['Name', 'Topic', 'Top_n_words']].head(20)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# View keywords for a specific topic (replace 0 with your topic number)\n",
        "topic_model.get_topic(0)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
